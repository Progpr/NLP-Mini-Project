{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import deque\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "import torch\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "import os\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-08-19T06:29:57.450748Z",
          "iopub.execute_input": "2022-08-19T06:29:57.451472Z",
          "iopub.status.idle": "2022-08-19T06:29:59.033423Z",
          "shell.execute_reply.started": "2022-08-19T06:29:57.451374Z",
          "shell.execute_reply": "2022-08-19T06:29:59.032423Z"
        },
        "trusted": true,
        "id": "e4e-ugsouXuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:29:59.037675Z",
          "iopub.execute_input": "2022-08-19T06:29:59.039019Z",
          "iopub.status.idle": "2022-08-19T06:29:59.291845Z",
          "shell.execute_reply.started": "2022-08-19T06:29:59.038979Z",
          "shell.execute_reply": "2022-08-19T06:29:59.290773Z"
        },
        "trusted": true,
        "id": "CZeNUzLPuXud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df.essay_id == '00066EA9880D']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:29:59.293616Z",
          "iopub.execute_input": "2022-08-19T06:29:59.293971Z",
          "iopub.status.idle": "2022-08-19T06:29:59.322459Z",
          "shell.execute_reply.started": "2022-08-19T06:29:59.293936Z",
          "shell.execute_reply": "2022-08-19T06:29:59.321553Z"
        },
        "trusted": true,
        "id": "Tov5tDlouXue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_cleaning(text):\n",
        "    \"\"\"\n",
        "    clear url/ not alpha/ fuck-bitch swear\n",
        "    \"\"\"\n",
        "    text = re.sub(r'https?://www\\.\\S+\\.cm', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z|\\s]', '', text)\n",
        "    text = re.sub(r'\\*+', 'swear', text)\n",
        "    return text\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>')\n",
        "    return html.sub(r'',text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    #emoticons\n",
        "    #symbols & pictographs\n",
        "    #transport & map symbols\n",
        "    #flags (iOS)\n",
        "    emoji_pattern = re.compile(\"[\"\\\n",
        "        u\"\\U0001F600-\\U0001F64F|\"\\\n",
        "        u\"\\U0001F300-\\U0001F5FF|\"\\\n",
        "        u\"\\U0001F680-\\U0001F6FF|\"\\\n",
        "        u\"\\U0001F1E0-\\U0001F1FF|\"\\\n",
        "        u\"\\U00002702-\\U000027B0|\"\\\n",
        "        u\"\\U000024C2-\\U0001F251\"\\\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# remove repeated characters\n",
        "def remove_multiplechars(text):\n",
        "    \"\"\"\n",
        "    for example, so we have “way” instead of “waaaayyyyy”\n",
        "    \"\"\"\n",
        "    text = re.sub(r'(.)\\1{3,}', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "def clean(df):\n",
        "    for col in ['discourse_text']:#,'selected_text']:\n",
        "        df[col] = df[col].astype(str).apply(lambda x:basic_cleaning(x))\n",
        "        df[col] = df[col].astype(str).apply(lambda x:remove_emoji(x))\n",
        "        df[col] = df[col].astype(str).apply(lambda x:remove_html(x))\n",
        "        df[col] = df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n",
        "    return df.sample(frac=1)\n",
        "\n",
        "train_df = clean(train_df)\n",
        "train_df_selection = train_df.sample(frac=1)\n",
        "X_tr = train_df_selection.discourse_text.values\n",
        "\n",
        "test_df = clean(test_df)\n",
        "test_df_selection = test_df.sample(frac=1)\n",
        "X_te = test_df_selection.discourse_text.values\n",
        "print(X_te.shape)\n",
        "print('clean Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:29:59.325278Z",
          "iopub.execute_input": "2022-08-19T06:29:59.325898Z",
          "iopub.status.idle": "2022-08-19T06:30:00.563623Z",
          "shell.execute_reply.started": "2022-08-19T06:29:59.325863Z",
          "shell.execute_reply": "2022-08-19T06:30:00.561642Z"
        },
        "trusted": true,
        "id": "O887oQ9ouXue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_tr1, uniques_tr1 = pd.factorize(train_df_selection.discourse_type, sort=True)\n",
        "y_tr2, uniques_tr2 = pd.factorize(train_df_selection.discourse_effectiveness, sort=True)\n",
        "print(uniques_tr1, uniques_tr2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:30:00.565001Z",
          "iopub.execute_input": "2022-08-19T06:30:00.565629Z",
          "iopub.status.idle": "2022-08-19T06:30:00.583469Z",
          "shell.execute_reply.started": "2022-08-19T06:30:00.565592Z",
          "shell.execute_reply": "2022-08-19T06:30:00.582472Z"
        },
        "trusted": true,
        "id": "YbJEvbLNuXul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_list = [\n",
        "    f'../input/feedback-prize-effectiveness/train/{f}' for f in os.listdir('../input/feedback-prize-effectiveness/train')\n",
        "] + [\n",
        "    f'../input/feedback-prize-effectiveness/test/{f}' for f in os.listdir('../input/feedback-prize-effectiveness/test')\n",
        "]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:33:39.418527Z",
          "iopub.execute_input": "2022-08-19T06:33:39.419189Z",
          "iopub.status.idle": "2022-08-19T06:33:39.428553Z",
          "shell.execute_reply.started": "2022-08-19T06:33:39.419151Z",
          "shell.execute_reply": "2022-08-19T06:33:39.427523Z"
        },
        "trusted": true,
        "id": "jb-cvJ2PuXum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# del word2id, id2word, wordid_freq\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:34:40.564783Z",
          "iopub.execute_input": "2022-08-19T06:34:40.565505Z",
          "iopub.status.idle": "2022-08-19T06:34:41.381253Z",
          "shell.execute_reply.started": "2022-08-19T06:34:40.565451Z",
          "shell.execute_reply": "2022-08-19T06:34:41.380097Z"
        },
        "trusted": true,
        "id": "WjCdrxhCuXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_g.stop_words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:34:52.899515Z",
          "iopub.execute_input": "2022-08-19T06:34:52.900567Z",
          "iopub.status.idle": "2022-08-19T06:34:52.907892Z",
          "shell.execute_reply.started": "2022-08-19T06:34:52.900503Z",
          "shell.execute_reply": "2022-08-19T06:34:52.906778Z"
        },
        "trusted": true,
        "id": "UCdanHw5uXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(data_g.wordid_freq), data_g.wordid_freq[data_g.word2id['unk']], len(data_g.stop_words), data_g.stop_words\n",
        "# sorted(data_g.wordid_freq.items(), key=lambda c:c[1], reverse=True)\n",
        "# # data_g.wordid_freq"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:34:41.382783Z",
          "iopub.execute_input": "2022-08-19T06:34:41.383377Z",
          "iopub.status.idle": "2022-08-19T06:34:41.388038Z",
          "shell.execute_reply.started": "2022-08-19T06:34:41.38334Z",
          "shell.execute_reply": "2022-08-19T06:34:41.387038Z"
        },
        "trusted": true,
        "id": "ZDxuRqbOuXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# help(gensim.models.callbacks.CallbackAny2Vec)."
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:34:41.390903Z",
          "iopub.execute_input": "2022-08-19T06:34:41.391542Z",
          "iopub.status.idle": "2022-08-19T06:34:41.396483Z",
          "shell.execute_reply.started": "2022-08-19T06:34:41.391506Z",
          "shell.execute_reply": "2022-08-19T06:34:41.395513Z"
        },
        "trusted": true,
        "id": "HKcenZduuXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# skp_model = word2Vec(\n",
        "#     sentences=None, corpus_file=None,\n",
        "#     vector_size=256, alpha=0.025, min_alpha=5e-5,\n",
        "#     window=5,  min_count=5, max_vocab_size=None,\n",
        "#     # drop hight freq word , random drop freq_rate > 1e-5\n",
        "#     sample=1e-5, seed=2022,\n",
        "#     # NSG\n",
        "#     negative=5, ns_exponent=0.75,\n",
        "#     sg=1, hs=0, workers=3,\n",
        "#     min_alpha=0.0001,\n",
        "#     epochs=5, sorted_vocab=1, batch_words=4096, compute_loss=True\n",
        "# )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:34:41.398085Z",
          "iopub.execute_input": "2022-08-19T06:34:41.39888Z",
          "iopub.status.idle": "2022-08-19T06:34:41.405146Z",
          "shell.execute_reply.started": "2022-08-19T06:34:41.398846Z",
          "shell.execute_reply": "2022-08-19T06:34:41.404075Z"
        },
        "trusted": true,
        "id": "xaRoduI9uXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch_pair(wordid_list, batch_size, window_size, idx, word_pair_queue):\n",
        "    while len(word_pair_queue) < batch_size:\n",
        "        if idx ==len(wordid_list):\n",
        "            idx=0\n",
        "        for _ in range(len(wordid_list)):\n",
        "            for i in range(max(idx - window_size, 0), min(idx + window_size+1, len(wordid_list)-1)):\n",
        "                w = wordid_list[idx]\n",
        "                v = wordid_list[i]\n",
        "                if idx == i:\n",
        "                    continue\n",
        "                word_pair_queue.append((w, v))\n",
        "            idx += 1\n",
        "        res_pair = []\n",
        "        for _ in range(batch_size):\n",
        "            res_pair.append(\n",
        "                word_pair_queue.popleft()\n",
        "            )\n",
        "        return res_pair\n",
        "\n",
        "def get_new_batch_pair(wordid_list, window_size):\n",
        "    word_pair_queue = []\n",
        "    for idx in range(len(wordid_list)):\n",
        "        for i in range(max(idx - window_size, 0), min(idx + window_size+1, len(wordid_list)-1)):\n",
        "            w = wordid_list[idx]\n",
        "            v = wordid_list[i]\n",
        "            if idx == i:\n",
        "                continue\n",
        "            if w == v:\n",
        "                continue\n",
        "            word_pair_queue.append((w, v))\n",
        "    return word_pair_queue\n",
        "\n",
        "\n",
        "wordid_list = data_g.file2id(files_list[20])\n",
        "# pos_pair = get_batch_pair(wordid_list, batch_size=32, window_size=3, idx=0, word_pair_queue=deque())\n",
        "pos_pair = get_new_batch_pair(wordid_list, window_size=5)\n",
        "neg_ = data_g.get_neg_samples(pos_pair, 5)\n",
        "print('get_neg_samples Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:35:02.714284Z",
          "iopub.execute_input": "2022-08-19T06:35:02.714999Z",
          "iopub.status.idle": "2022-08-19T06:35:02.735182Z",
          "shell.execute_reply.started": "2022-08-19T06:35:02.714961Z",
          "shell.execute_reply": "2022-08-19T06:35:02.734016Z"
        },
        "trusted": true,
        "id": "xYa-EltDuXuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_pair = []\n",
        "# neg_ = []\n",
        "# for f in tqdm(files_list):\n",
        "#     wordid_list = data_g.file2id(f)\n",
        "#     tmp_pair = get_new_batch_pair(wordid_list, window_size=5)\n",
        "#     print(len(tmp_pair))\n",
        "#     tmp_pair = data_g.subsample(tmp_pair)\n",
        "#     print(len(tmp_pair))\n",
        "#     pos_pair.extend(tmp_pair)\n",
        "#     neg_.extend(data_g.get_neg_samples(tmp_pair,  5))\n",
        "#     break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:33:17.95577Z",
          "iopub.status.idle": "2022-08-19T06:33:17.956778Z",
          "shell.execute_reply.started": "2022-08-19T06:33:17.956505Z",
          "shell.execute_reply": "2022-08-19T06:33:17.95653Z"
        },
        "trusted": true,
        "id": "LL62dTwpuXuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data_g.wordid_freq))\n",
        "skp_model = skipGramModel(vocab_size=len(data_g.wordid_freq), embed_size=256)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:44:53.824346Z",
          "iopub.execute_input": "2022-08-19T06:44:53.825478Z",
          "iopub.status.idle": "2022-08-19T06:44:53.945506Z",
          "shell.execute_reply.started": "2022-08-19T06:44:53.825398Z",
          "shell.execute_reply": "2022-08-19T06:44:53.944415Z"
        },
        "trusted": true,
        "id": "gsga48UjuXup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_pair = []\n",
        "neg_ = []\n",
        "for f in tqdm(files_list):\n",
        "    wordid_list = data_g.file2id(f)\n",
        "    tmp_pair = get_new_batch_pair(wordid_list, window_size=5)\n",
        "    tmp_pair = data_g.subsample(tmp_pair)\n",
        "    pos_pair.extend(tmp_pair)\n",
        "    neg_.extend(data_g.get_neg_samples(tmp_pair,  5))\n",
        "\n",
        "print('get_new_batch_pair Done')\n",
        "pos_pair = np.array(pos_pair)\n",
        "\n",
        "lr_ = 0.015 # 1e-3\n",
        "print('--'*25, '\\nlr:', lr_ )\n",
        "skp_model.compile(\n",
        "    loss=nn.CrossEntropyLoss(),\n",
        "    optimizer=SparseAdam,\n",
        "    learning_rate=lr_,\n",
        "    metrics=[accuracy_score],\n",
        "    verbose=100\n",
        ")\n",
        "skp_model.fit(pos_pair[:, 0], pos_pair[:, 1], np.array(neg_), batch_size=2048, epochs=3)\n",
        "\n",
        "# pos_pair = []\n",
        "# neg_ = []\n",
        "# for f in tqdm(files_list):\n",
        "#     wordid_list = data_g.file2id(f)\n",
        "#     tmp_pair = get_new_batch_pair(wordid_list, window_size=5)\n",
        "#     tmp_pair = data_g.subsample(tmp_pair)\n",
        "#     pos_pair.extend(tmp_pair)\n",
        "#     neg_.extend(data_g.get_neg_samples(tmp_pair,  5))\n",
        "\n",
        "# print('get_new_batch_pair Done')\n",
        "# pos_pair = np.array(pos_pair)\n",
        "# lr_ = 1e-3\n",
        "# print('--'*25, '\\nlr:', lr_ )\n",
        "# skp_model.optim = SparseAdam(skp_model.parameters(), lr=lr_)\n",
        "# skp_model.fit(pos_pair[:, 0], pos_pair[:, 1], np.array(neg_), batch_size=2048, epochs=1)\n",
        "\n",
        "# lr_ = 1e-5\n",
        "# print('--'*25, '\\nlr:', lr_ )\n",
        "# skp_model.optim = SparseAdam(skp_model.parameters(), lr=lr_)\n",
        "# skp_model.fit(pos_pair[:, 0], pos_pair[:, 1], np.array(neg_), batch_size=2048, epochs=1)\n",
        "# print('skp_model Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:44:54.895375Z",
          "iopub.execute_input": "2022-08-19T06:44:54.895762Z",
          "iopub.status.idle": "2022-08-19T06:51:51.306867Z",
          "shell.execute_reply.started": "2022-08-19T06:44:54.89573Z",
          "shell.execute_reply": "2022-08-19T06:51:51.305862Z"
        },
        "trusted": true,
        "id": "VG9PWjnTuXup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pos_pair, neg_\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:52:03.943399Z",
          "iopub.execute_input": "2022-08-19T06:52:03.943782Z",
          "iopub.status.idle": "2022-08-19T06:52:03.961836Z",
          "shell.execute_reply.started": "2022-08-19T06:52:03.943751Z",
          "shell.execute_reply": "2022-08-19T06:52:03.960495Z"
        },
        "trusted": true,
        "id": "I4f1SHzCuXup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_encode(texts, maxlen=128):\n",
        "    all_ids = []\n",
        "    for i in tqdm(range(len(texts))):\n",
        "        tmp_ids = data_g.context2id(texts[i])\n",
        "        tmp_ids = tmp_ids[:maxlen] + [0] * (maxlen - len(tmp_ids))\n",
        "        all_ids.append(tmp_ids)\n",
        "    return np.array(all_ids)\n",
        "\n",
        "X = my_encode(\n",
        "    X_tr,\n",
        "    maxlen=128\n",
        ")\n",
        "\n",
        "X_te_enc = my_encode(\n",
        "    X_te,\n",
        "    maxlen=128\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T06:52:08.581149Z",
          "iopub.execute_input": "2022-08-19T06:52:08.581943Z",
          "iopub.status.idle": "2022-08-19T06:52:10.504726Z",
          "shell.execute_reply.started": "2022-08-19T06:52:08.581902Z",
          "shell.execute_reply": "2022-08-19T06:52:10.503728Z"
        },
        "trusted": true,
        "id": "8baEpDoXuXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torch.optim import AdamW, Adam, RMSprop\n",
        "from sklearn.metrics import accuracy_score\n",
        "import typing  as typ\n",
        "\n",
        "embed_1 = skp_model.w_embed.weight.data.cpu().numpy()\n",
        "embed_2 = skp_model.v_embed.weight.data.cpu().numpy()\n",
        "embed = (embed_1 + embed_2) / 2\n",
        "\n",
        "\n",
        "class compDataset(Dataset):\n",
        "    def __init__(self, encodings, label_i, label_j):\n",
        "        self.encodings = encodings\n",
        "        self.label_i = label_i\n",
        "        self.label_j = label_j\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids =  torch.tensor(self.encodings[idx]).long()\n",
        "        s_idx = torch.Tensor(range(len(ids)))\n",
        "        s_mask = torch.ones((len(ids), 256)).float()\n",
        "        if sum(ids != 0):\n",
        "            st = int(s_idx[ids != 0].max(dim=0).values + 1)\n",
        "            if st < len(ids):\n",
        "                s_mask[st:] = 1e-9\n",
        "        item = {'input_ids': ids}\n",
        "        item['mask'] = s_mask\n",
        "        item['label_i'] = torch.tensor(int(self.label_i[idx]))\n",
        "        item['label_j'] = torch.tensor(int(self.label_j[idx]))\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label_i)\n",
        "\n",
        "\n",
        "\n",
        "class multiTaskModel(nn.Module):\n",
        "    def __init__(self, num_labels_i, num_labels_j):\n",
        "        super(multiTaskModel, self).__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(\n",
        "            torch.Tensor(embed).float(), freeze=True\n",
        "        )\n",
        "        self.lstm_layer = nn.LSTM(input_size=256, hidden_size=128, bidirectional=True)\n",
        "        self.lstm_layer2 = nn.LSTM(input_size=128*2, hidden_size=64, bidirectional=True)\n",
        "        self.drop1 = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(64*2, 32)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop2 = nn.Dropout(0.3)\n",
        "        self.sgm1 = nn.Sigmoid()\n",
        "        self.sgm2 = nn.Sigmoid()\n",
        "        self.classifier_i = nn.Linear(32, num_labels_i)\n",
        "        self.classifier_j = nn.Linear(32, num_labels_j)\n",
        "        self._reinitialize()\n",
        "\n",
        "    def _reinitialize(self):\n",
        "        \"\"\"\n",
        "        Tensorflow/Keras-like initialization\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        for name, p in self.named_parameters():\n",
        "            if 'lstm' in name:\n",
        "                if 'weight_ih' in name:\n",
        "                    nn.init.xavier_uniform_(p.data)\n",
        "                elif 'weight_hh' in name:\n",
        "                    nn.init.orthogonal_(p.data)\n",
        "                elif 'bias_ih' in name:\n",
        "                    p.data.fill_(0)\n",
        "                    # Set forget-gate bias to 1\n",
        "                    n = p.size(0)\n",
        "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
        "                elif 'bias_hh' in name:\n",
        "                    p.data.fill_(0)\n",
        "            elif 'fc' in name:\n",
        "                if 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(p.data)\n",
        "                elif 'bias' in name:\n",
        "                    p.data.fill_(0)\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        out = self.embed(inputs)\n",
        "        out = torch.mul(out, mask)\n",
        "        out,(h,c) = self.lstm_layer(out)\n",
        "        out,(h,c) = self.lstm_layer2(out)\n",
        "        out = out.max(axis=1).values\n",
        "        out=self.drop1(out)\n",
        "        out=self.fc1(out)\n",
        "        out=self.relu(out)\n",
        "        out=self.drop2(out)\n",
        "#         logits_i = self.sgm1(torch.clamp(self.classifier_i(out), min=-10, max=10))\n",
        "#         logits_j = self.sgm2(torch.clamp(self.classifier_j(out), min=-10, max=10))\n",
        "        logits_i = self.classifier_i(out)\n",
        "        logits_j = self.classifier_j(out)\n",
        "        return logits_i, logits_j\n",
        "\n",
        "    def compile(self,\n",
        "                loss: typ.Callable,\n",
        "                optimizer: typ.Callable,\n",
        "                learning_rate: float,\n",
        "                metrics: typ.List[typ.Callable],\n",
        "                verbose: int=100):\n",
        "        self.verbose = verbose\n",
        "        b=torch.cuda.is_available()\n",
        "        if(b):\n",
        "            self.device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "\n",
        "        self.to(self.device)\n",
        "        self.loss_fn = loss\n",
        "        self.optim = optimizer(self.parameters(), lr=learning_rate)\n",
        "        self.metrics = metrics\n",
        "        self.iter_num = 0\n",
        "\n",
        "    def _training_step(self, x, y_i, y_j, mask):\n",
        "        x = x.to(self.device)\n",
        "        y_i = y_i.to(self.device)\n",
        "        y_j = y_j.to(self.device)\n",
        "        mask = mask.to(self.device)\n",
        "        self.epoch_steps += 1\n",
        "        self.epoch_samples += len(y_i)\n",
        "        # 正向传播\n",
        "        self.optim.zero_grad()\n",
        "        pred_i, pred_j =  self.forward(x, mask)\n",
        "        loss_i = self.loss_fn(pred_i, y_i)\n",
        "        loss_j = self.loss_fn(pred_j, y_j)\n",
        "        loss = 0.2 * loss_i + 0.8 * loss_j\n",
        "\n",
        "        pred_i = pred_i.cpu().detach().numpy()\n",
        "        pred_j = pred_j.cpu().detach().numpy()\n",
        "        self.epoch_loss += loss.detach().cpu().numpy()\n",
        "        self.epoch_focus_loss += loss_j.detach().cpu().numpy()\n",
        "        self.epoch_right_samples += np.sum(np.argmax(pred_i, axis=1) == y_i.cpu().detach().numpy() )\n",
        "        self.epoch_right_samples_j += np.sum(np.argmax(pred_j, axis=1) == y_j.cpu().detach().numpy() )\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "\n",
        "        self.optim.step()\n",
        "        self.iter_num += 1\n",
        "        if(self.iter_num % self.verbose == 0):\n",
        "            metric_str = ''\n",
        "            for metric_fn in self.metrics:\n",
        "                metric_res_i = metric_fn(np.argmax(pred_i, axis=1), y_i.cpu().detach().numpy())\n",
        "                metric_res_j = metric_fn(np.argmax(pred_j, axis=1), y_j.cpu().detach().numpy())\n",
        "                metric_str += f'{metric_fn.__name__}-i : {metric_res_i:.3f}, {metric_fn.__name__}-j : {metric_res_j:.3f},'\n",
        "\n",
        "            print(f\"[ iter_num-{self.iter_num} ]: loss: {loss:.5f} loss_j: {loss_j:.5f} \" + metric_str)\n",
        "        return self.epoch_loss, self.epoch_focus_loss\n",
        "\n",
        "    def train_one_epoch(self, dataloader, ep_idx=0):\n",
        "        self.epoch_loss = 0\n",
        "        self.epoch_samples = 0\n",
        "        self.epoch_right_samples = 0\n",
        "        self.epoch_right_samples_j = 0\n",
        "        self.epoch_steps = 0\n",
        "        self.epoch_focus_loss = 0\n",
        "\n",
        "        tqdm_bar = tqdm(dataloader)\n",
        "        for batch in tqdm_bar:\n",
        "            loss_tt, loss = self._training_step(batch['input_ids'], batch['label_i'], batch['label_j'], batch['mask'])\n",
        "            loss_p = loss / self.epoch_steps\n",
        "            loss_tt = loss_tt / self.epoch_steps\n",
        "            tqdm_bar.set_description(f'[ Train-Epoch {ep_idx} ]')\n",
        "            tqdm_bar.set_postfix(loss=f'total:{loss_tt:.5f} focus_loss:{loss_p:.5f}')\n",
        "            tqdm_bar.update()\n",
        "\n",
        "\n",
        "        self.epoch_loss /= self.epoch_steps\n",
        "        self.epoch_focus_loss /= self.epoch_steps\n",
        "        acc_i = self.epoch_right_samples / self.epoch_samples\n",
        "        acc_j = self.epoch_right_samples_j / self.epoch_samples\n",
        "        print(f\"[ epoch: {ep_idx} ] loss_tt: {self.epoch_loss:.5f}, loss_j: {self.epoch_focus_loss:.5f}, acci: {acc_i:.3f}, accj: {acc_j:.3f}\")\n",
        "\n",
        "    def _valitate_step(self, x, y_i, y_j, mask):\n",
        "        with torch.no_grad():\n",
        "            x = x.to(self.device)\n",
        "            y_i = y_i.to(self.device)\n",
        "            y_j = y_j.to(self.device)\n",
        "            mask = mask.to(self.device)\n",
        "            self.epoch_steps += 1\n",
        "            self.epoch_samples += len(y_i)\n",
        "            pred_i, pred_j =  self.forward(x, mask)\n",
        "            loss = self.loss_fn(pred_i, y_i) + self.loss_fn(pred_j, y_j)\n",
        "\n",
        "            pred_i = pred_i.cpu().detach().numpy()\n",
        "            pred_j = pred_j.cpu().detach().numpy()\n",
        "            self.epoch_loss += loss.detach().cpu().numpy()\n",
        "            self.epoch_right_samples += np.sum(np.argmax(pred_i, axis=1) == y_i.cpu().detach().numpy() )\n",
        "            self.epoch_right_samples_j += np.sum(np.argmax(pred_j, axis=1) == y_j.cpu().detach().numpy() )\n",
        "            self.iter_num += 1\n",
        "            if(self.iter_num % self.verbose == 0):\n",
        "                metric_str = ''\n",
        "                for metric_fn in self.metrics:\n",
        "                    metric_res_i = metric_fn(np.argmax(pred_i, axis=1), y_i.cpu().detach().numpy())\n",
        "                    metric_res_j = metric_fn(np.argmax(pred_j, axis=1), y_j.cpu().detach().numpy())\n",
        "                    metric_str += f'{metric_fn.__name__}-i : {metric_res_i:.3f}, {metric_fn.__name__}-j : {metric_res_j:.3f},'\n",
        "\n",
        "                print(f\"[ iter_num-{self.iter_num} ]: loss: {loss:.5f}  \" + metric_str)\n",
        "\n",
        "    def valitate_one_epoch(self, dataloader, ep_idx=0):\n",
        "        self.epoch_loss = 0\n",
        "        self.epoch_samples = 0\n",
        "        self.epoch_right_samples = 0\n",
        "        self.epoch_right_samples_j = 0\n",
        "        self.epoch_steps = 0\n",
        "        for batch in tqdm(dataloader):\n",
        "            self._valitate_step(batch['input_ids'], batch['label_i'], batch['label_j'], batch['mask'])\n",
        "\n",
        "        self.epoch_loss /= self.epoch_steps\n",
        "        acc_i = self.epoch_right_samples / self.epoch_samples\n",
        "        acc_j = self.epoch_right_samples_j / self.epoch_samples\n",
        "        print(\"-------------------------------\")\n",
        "        print(f\"[ epoch: {ep_idx}-valitation ] loss_tt: {self.epoch_loss:.5f}, acci: {acc_i:.3f}, accj: {acc_j:.3f}\")\n",
        "        print(\"-------------------------------\")\n",
        "\n",
        "    def fit(self, X, yi, yj,\n",
        "            batch_size: int=32,\n",
        "            epochs: int=5,\n",
        "            validation_split: float=0.1\n",
        "            ):\n",
        "\n",
        "        sp = StratifiedShuffleSplit(n_splits=epochs, test_size=int(len(yi)*validation_split))\n",
        "        for idx, (tr_idx, te_idx) in  enumerate(sp.split(X, yi)):\n",
        "            tr_data = compDataset(X[tr_idx, :], yi[tr_idx], yj[tr_idx])\n",
        "            val_data = compDataset(X[te_idx, :], yi[te_idx], yj[te_idx])\n",
        "            train_loader = DataLoader(tr_data, batch_size=batch_size, shuffle=True)\n",
        "            val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "            self.train_one_epoch(train_loader, idx+1)\n",
        "            self.valitate_one_epoch(val_dataloader, idx+1)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:26:05.288611Z",
          "iopub.execute_input": "2022-08-19T07:26:05.289036Z",
          "iopub.status.idle": "2022-08-19T07:26:05.342746Z",
          "shell.execute_reply.started": "2022-08-19T07:26:05.28899Z",
          "shell.execute_reply": "2022-08-19T07:26:05.341605Z"
        },
        "trusted": true,
        "id": "jOiKZ7n9uXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = multiTaskModel(len(uniques_tr1), len(uniques_tr2))\n",
        "model.compile(\n",
        "    loss=nn.CrossEntropyLoss(),\n",
        "    optimizer=AdamW, # RMSprop, #\n",
        "    learning_rate=1e-3 , #5e-4\n",
        "    metrics=[accuracy_score],\n",
        "    verbose=100\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:26:05.361199Z",
          "iopub.execute_input": "2022-08-19T07:26:05.36181Z",
          "iopub.status.idle": "2022-08-19T07:26:05.387513Z",
          "shell.execute_reply.started": "2022-08-19T07:26:05.36177Z",
          "shell.execute_reply": "2022-08-19T07:26:05.386625Z"
        },
        "trusted": true,
        "id": "eSei9gifuXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "model.fit(X, y_tr1, y_tr2,\n",
        "        batch_size=batch_size,\n",
        "        epochs=20,\n",
        "        validation_split=0.2\n",
        ")\n",
        "\n",
        "# lr_ = 5e-5\n",
        "# print('--'*25, '\\nlr:', lr_ )\n",
        "# model.optim = AdamW(model.parameters(), lr=lr_) #RMSprop(model.parameters(), lr=lr_)#\n",
        "# model.fit(X, y_tr1, y_tr2,\n",
        "#         batch_size=batch_size,\n",
        "#         epochs=10,\n",
        "#         validation_split=0.2\n",
        "# )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:26:05.38931Z",
          "iopub.execute_input": "2022-08-19T07:26:05.389989Z",
          "iopub.status.idle": "2022-08-19T07:39:06.484347Z",
          "shell.execute_reply.started": "2022-08-19T07:26:05.389952Z",
          "shell.execute_reply": "2022-08-19T07:39:06.483263Z"
        },
        "trusted": true,
        "id": "9qOBghw6uXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(teloaders):\n",
        "    pred_i_list = []\n",
        "    pred_j_list = []\n",
        "    for batch in teloaders:\n",
        "        x, y_i, y_j, mask = batch['input_ids'], batch['label_i'], batch['label_j'], batch['mask']\n",
        "        with torch.no_grad():\n",
        "            x = x.to(model.device)\n",
        "            y_i = y_i.to(model.device)\n",
        "            y_j = y_j.to(model.device)\n",
        "            mask = mask.to(model.device)\n",
        "            pred_i, pred_j =  model.forward(x, mask)\n",
        "            pred_i = pred_i.cpu().detach().numpy()\n",
        "            pred_j = pred_j.cpu().detach().numpy()\n",
        "            pred_i_list.append(pred_i)\n",
        "            pred_j_list.append(pred_j)\n",
        "    return np.concatenate(pred_i_list), np.concatenate(pred_j_list)\n",
        "\n",
        "\n",
        "\n",
        "final_test_data = compDataset(X_te_enc, [0]*len(X_te_enc), [0]*len(X_te_enc))\n",
        "teloaders = DataLoader(final_test_data, batch_size=batch_size, shuffle=False)\n",
        "pred_i_out, pred_j_out = predict(teloaders)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:06.512717Z",
          "iopub.execute_input": "2022-08-19T07:39:06.513528Z",
          "iopub.status.idle": "2022-08-19T07:39:06.535111Z",
          "shell.execute_reply.started": "2022-08-19T07:39:06.513468Z",
          "shell.execute_reply": "2022-08-19T07:39:06.534146Z"
        },
        "trusted": true,
        "id": "xLo1tPO-uXuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "PQrYAKy4uXur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with torch.no_grad():\n",
        "#     aa = pred_j_out + np.abs(pred_j_out.min(axis=1, keepdims=True))*1.25\n",
        "#     aa = aa[:, [2, 0, 1]]\n",
        "#     my_norm_pred = np.clip(np.exp(aa) / np.exp(aa).sum(axis=1, keepdims=True), a_min=1e-15, a_max=1-1e-15)\n",
        "#     norm_pred = np.array([\n",
        "#             [0.0142, 0.3949, 0.5909],\n",
        "#             [0.0380, 0.8436, 0.1184],\n",
        "#             [0.0301, 0.7221, 0.2478],\n",
        "#             [0.0692, 0.6866, 0.2441],\n",
        "#             [0.0585, 0.6580, 0.2835],\n",
        "#             [0.0217, 0.4150, 0.5633],\n",
        "#             [0.0200, 0.3050, 0.6751],\n",
        "#             [0.0309, 0.6665, 0.3026],\n",
        "#             [0.0299, 0.4357, 0.5344],\n",
        "#             [0.0237, 0.5962, 0.3801]\n",
        "#             ])\n",
        "#     print('lab 0.61 loss:',model.loss_fn(\n",
        "#         torch.Tensor(norm_pred).float(),\n",
        "#         torch.LongTensor([2, 1, 1, 1, 1, 2, 2, 1, 2, 1])))\n",
        "#     print('My loss:',model.loss_fn(\n",
        "#         torch.Tensor(my_norm_pred).float(),\n",
        "#         torch.LongTensor([2, 1, 1, 1, 1, 2, 2, 1, 2, 1])))\n",
        "\n",
        "# X_te_enc.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:14.711421Z",
          "iopub.execute_input": "2022-08-19T07:39:14.712419Z",
          "iopub.status.idle": "2022-08-19T07:39:14.729079Z",
          "shell.execute_reply.started": "2022-08-19T07:39:14.712375Z",
          "shell.execute_reply": "2022-08-19T07:39:14.727917Z"
        },
        "trusted": true,
        "id": "RqgAew9FuXur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit"
      ],
      "metadata": {
        "id": "QU5036Z0uXur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/sample_submission.csv')\n",
        "print(uniques_tr2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:24.377107Z",
          "iopub.execute_input": "2022-08-19T07:39:24.377487Z",
          "iopub.status.idle": "2022-08-19T07:39:24.390913Z",
          "shell.execute_reply.started": "2022-08-19T07:39:24.377456Z",
          "shell.execute_reply": "2022-08-19T07:39:24.389724Z"
        },
        "trusted": true,
        "id": "cEDcrkGQuXur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_df = pd.DataFrame({\"discourse_id\": test_df_selection['discourse_id']})\n",
        "out_df[['Adequate', 'Effective', 'Ineffective']] = pred_j_out + np.abs(pred_j_out.min(axis=1, keepdims=True))*1.25\n",
        "sub_df_f = pd.DataFrame(sub_df['discourse_id']).merge(out_df[['discourse_id', 'Ineffective', 'Adequate', 'Effective']],\n",
        "                             on='discourse_id', how='left')\n",
        "print(sub_df_f.isna().sum().sum())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:25.068835Z",
          "iopub.execute_input": "2022-08-19T07:39:25.069915Z",
          "iopub.status.idle": "2022-08-19T07:39:25.086395Z",
          "shell.execute_reply.started": "2022-08-19T07:39:25.069866Z",
          "shell.execute_reply": "2022-08-19T07:39:25.085102Z"
        },
        "trusted": true,
        "id": "svAMrF69uXus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df_f.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:26.874401Z",
          "iopub.execute_input": "2022-08-19T07:39:26.875096Z",
          "iopub.status.idle": "2022-08-19T07:39:26.881796Z",
          "shell.execute_reply.started": "2022-08-19T07:39:26.875059Z",
          "shell.execute_reply": "2022-08-19T07:39:26.880721Z"
        },
        "trusted": true,
        "id": "rua7ngZIuXus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df_f"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:27.389973Z",
          "iopub.execute_input": "2022-08-19T07:39:27.390904Z",
          "iopub.status.idle": "2022-08-19T07:39:27.404198Z",
          "shell.execute_reply.started": "2022-08-19T07:39:27.390856Z",
          "shell.execute_reply": "2022-08-19T07:39:27.402903Z"
        },
        "trusted": true,
        "id": "ZbzvaV7wuXus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_df_f[['Ineffective','Adequate', 'Effective']].values/sub_df_f[['Adequate', 'Effective', 'Ineffective']].values.sum(axis=1, keepdims=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-19T07:39:36.641192Z",
          "iopub.execute_input": "2022-08-19T07:39:36.642197Z",
          "iopub.status.idle": "2022-08-19T07:39:36.652566Z",
          "shell.execute_reply.started": "2022-08-19T07:39:36.642157Z",
          "shell.execute_reply": "2022-08-19T07:39:36.651435Z"
        },
        "trusted": true,
        "id": "f5fRZ88YuXus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6D8jyjruXus"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}